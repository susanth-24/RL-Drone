{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymunk\n",
        "!pip install pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jJfz9GqsU5H",
        "outputId": "5ad2fb7d-4427-4733-c232-1306eb23d0a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pymunk\n",
            "  Downloading pymunk-6.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (996 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.5/996.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from pymunk) (1.15.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.15.0->pymunk) (2.21)\n",
            "Installing collected packages: pymunk\n",
            "Successfully installed pymunk-6.4.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (2.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk import Vec2d\n",
        "import numpy as np\n",
        "import pygame"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VyklGiZTsamH",
        "outputId": "e83c463a-c7ca-4b0f-9bc2-08578282618f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.3.0 (SDL 2.24.2, Python 3.10.12)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Drone():\n",
        "\n",
        "    def __init__(self, x, y, angle, height, width, mass_f, mass_l, mass_r, space):\n",
        "        distance_between_joints = height/2 - 3\n",
        "        self.drone_radius = width/2 - height/2\n",
        "\n",
        "        #Drone's frame properties\n",
        "        self.frame_shape = pymunk.Poly.create_box(None, size=(width, height/2))\n",
        "        frame_moment_of_inertia = pymunk.moment_for_poly(mass_f, self.frame_shape.get_vertices())\n",
        "\n",
        "        frame_body = pymunk.Body(mass_f, frame_moment_of_inertia, body_type=pymunk.Body.DYNAMIC)\n",
        "        frame_body.position = x, y\n",
        "        frame_body.angle = angle\n",
        "\n",
        "        self.frame_shape.body = frame_body\n",
        "        self.frame_shape.sensor = True\n",
        "        self.frame_shape.color = pygame.Color((66, 135, 245))\n",
        "\n",
        "        space.add(frame_body, self.frame_shape)\n",
        "\n",
        "        #Drone's left motor properties\n",
        "        self.left_motor_shape = pymunk.Poly.create_box(None, size=(height, height))\n",
        "        left_motor_moment_of_inertia = pymunk.moment_for_poly(mass_l, self.left_motor_shape.get_vertices())\n",
        "\n",
        "        left_motor_body = pymunk.Body(mass_l, left_motor_moment_of_inertia, body_type=pymunk.Body.DYNAMIC)\n",
        "        left_motor_body.position = np.cos(angle+np.pi)*self.drone_radius+x, np.sin(angle+np.pi)*self.drone_radius+y\n",
        "        left_motor_body.angle = angle\n",
        "\n",
        "        self.left_motor_shape.body = left_motor_body\n",
        "        self.left_motor_shape.sensor = True\n",
        "        self.left_motor_shape.color = pygame.Color((33, 93, 191))\n",
        "\n",
        "        space.add(left_motor_body, self.left_motor_shape)\n",
        "\n",
        "        #Drone's right motor properties\n",
        "        self.right_motor_shape = pymunk.Poly.create_box(None, size=(height, height))\n",
        "        right_motor_moment_of_inertia = pymunk.moment_for_poly(mass_r, self.right_motor_shape.get_vertices())\n",
        "\n",
        "        right_motor_body = pymunk.Body(mass_r, right_motor_moment_of_inertia, body_type=pymunk.Body.DYNAMIC)\n",
        "        right_motor_body.position = np.cos(angle)*self.drone_radius+x, np.sin(angle)*self.drone_radius+y\n",
        "        right_motor_body.angle = angle\n",
        "\n",
        "        self.right_motor_shape.body = right_motor_body\n",
        "        self.right_motor_shape.sensor = True\n",
        "        self.right_motor_shape.color = pygame.Color((33, 93, 191))\n",
        "\n",
        "        space.add(right_motor_body, self.right_motor_shape)\n",
        "\n",
        "        #Properties of the joints\n",
        "        motor_point = (-distance_between_joints, 0)\n",
        "        frame_point = (-self.drone_radius - distance_between_joints, 0)\n",
        "        self.left_1 = pymunk.PivotJoint(self.left_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.left_1.error_bias = 0\n",
        "        space.add(self.left_1)\n",
        "\n",
        "        motor_point = (0, 0)\n",
        "        frame_point = (-self.drone_radius, 0)\n",
        "        self.left_2 = pymunk.PivotJoint(self.left_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.left_2.error_bias = 0\n",
        "        space.add(self.left_2)\n",
        "\n",
        "        motor_point = (distance_between_joints, 0)\n",
        "        frame_point = (-self.drone_radius + distance_between_joints, 0)\n",
        "        self.left_3 = pymunk.PivotJoint(self.left_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.left_3.error_bias = 0\n",
        "        space.add(self.left_3)\n",
        "\n",
        "        motor_point = (-distance_between_joints, 0)\n",
        "        frame_point = (self.drone_radius - distance_between_joints, 0)\n",
        "        self.right_1 = pymunk.PivotJoint(self.right_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.right_1.error_bias = 0\n",
        "        space.add(self.right_1)\n",
        "\n",
        "        motor_point = (0, 0)\n",
        "        frame_point = (self.drone_radius, 0)\n",
        "        self.right_2 = pymunk.PivotJoint(self.right_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.right_2.error_bias = 0\n",
        "        space.add(self.right_2)\n",
        "\n",
        "        motor_point = (distance_between_joints, 0)\n",
        "        frame_point = (self.drone_radius + distance_between_joints, 0)\n",
        "        self.right_3 = pymunk.PivotJoint(self.right_motor_shape.body, self.frame_shape.body, motor_point, frame_point)\n",
        "        self.right_3.error_bias = 0\n",
        "        space.add(self.right_3)\n",
        "\n",
        "    def change_positions(self, x, y, space):\n",
        "        self.frame_shape.body.position = x, y\n",
        "        space.reindex_shapes_for_body(self.frame_shape.body)\n",
        "\n",
        "        angle = self.frame_shape.body.angle\n",
        "\n",
        "        self.left_motor_shape.body.position = np.cos(angle+np.pi)*self.drone_radius+x, np.sin(angle+np.pi)*self.drone_radius+y\n",
        "        space.reindex_shapes_for_body(self.left_motor_shape.body)\n",
        "\n",
        "        self.right_motor_shape.body.position = np.cos(angle)*self.drone_radius+x, np.sin(angle)*self.drone_radius+y\n",
        "        space.reindex_shapes_for_body(self.right_motor_shape.body)\n"
      ],
      "metadata": {
        "id": "q5oui1ciga9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pygame\n",
        "from pygame.locals import (QUIT, KEYDOWN, K_ESCAPE)\n",
        "import sys\n",
        "\n",
        "def pygame_events(space, myenv, change_target):\n",
        "    for event in pygame.event.get():\n",
        "        if event.type == QUIT:\n",
        "            pygame.quit()\n",
        "            sys.exit()\n",
        "\n",
        "        if change_target == True and event.type == pygame.MOUSEBUTTONUP:\n",
        "            x, y = pygame.mouse.get_pos()\n",
        "            myenv.change_target_point(x, 800-y)"
      ],
      "metadata": {
        "id": "0eXr1ogeFhEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "from gym import spaces\n",
        "from gym.utils import seeding\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "class Drone2dEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    render_sim: (bool) if true, a graphic is generated\n",
        "    render_path: (bool) if true, the drone's path is drawn\n",
        "    render_shade: (bool) if true, the drone's shade is drawn\n",
        "    shade_distance: (int) distance between consecutive drone's shades\n",
        "    n_steps: (int) number of time steps\n",
        "    n_fall_steps: (int) the number of initial steps for which the drone can't do anything\n",
        "    change_target: (bool) if true, mouse click change target positions\n",
        "    initial_throw: (bool) if true, the drone is initially thrown with random force\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, render_sim=False, render_path=True, render_shade=True, shade_distance=70,\n",
        "                 n_steps=500, n_fall_steps=10, change_target=False, initial_throw=True):\n",
        "\n",
        "        self.render_sim = render_sim\n",
        "        self.render_path = render_path\n",
        "        self.render_shade = render_shade\n",
        "\n",
        "        if self.render_sim is True:\n",
        "            self.init_pygame()\n",
        "            self.flight_path = []\n",
        "            self.drop_path = []\n",
        "            self.path_drone_shade = []\n",
        "\n",
        "        self.init_pymunk()\n",
        "\n",
        "        #Parameters\n",
        "        self.max_time_steps = n_steps\n",
        "        self.stabilisation_delay = n_fall_steps\n",
        "        self.drone_shade_distance = shade_distance\n",
        "        self.froce_scale = 1000\n",
        "        self.initial_throw = initial_throw\n",
        "        self.change_target = change_target\n",
        "\n",
        "        #Initial values\n",
        "        self.first_step = True\n",
        "        self.done = False\n",
        "        self.info = {}\n",
        "        self.current_time_step = 0\n",
        "        self.left_force = -1\n",
        "        self.right_force = -1\n",
        "\n",
        "        #Generating target position\n",
        "        self.x_target = random.uniform(50, 750)\n",
        "        self.y_target = random.uniform(50, 750)\n",
        "\n",
        "        #Defining spaces for action and observation\n",
        "        min_action = np.array([-1, -1], dtype=np.float32)\n",
        "        max_action = np.array([1, 1], dtype=np.float32)\n",
        "        self.action_space = spaces.Box(low=min_action, high=max_action, dtype=np.float32)\n",
        "\n",
        "        min_observation = np.array([-1, -1, -1, -1, -1, -1, -1, -1], dtype=np.float32)\n",
        "        max_observation = np.array([1, 1, 1, 1, 1, 1, 1, 1], dtype=np.float32)\n",
        "        self.observation_space = spaces.Box(low=min_observation, high=max_observation, dtype=np.float32)\n",
        "\n",
        "    def init_pygame(self):\n",
        "        pygame.init()\n",
        "        self.screen = pygame.display.set_mode((800, 800))\n",
        "        pygame.display.set_caption(\"Drone2d Environment\")\n",
        "        self.clock = pygame.time.Clock()\n",
        "\n",
        "        script_dir = os.path.dirname(__file__)\n",
        "        icon_path = os.path.join(\"img\", \"icon.png\")\n",
        "        icon_path = os.path.join(script_dir, icon_path)\n",
        "        pygame.display.set_icon(pygame.image.load(icon_path))\n",
        "\n",
        "        img_path = os.path.join(\"img\", \"shade.png\")\n",
        "        img_path = os.path.join(script_dir, img_path)\n",
        "        self.shade_image = pygame.image.load(img_path)\n",
        "\n",
        "    def init_pymunk(self):\n",
        "        self.space = pymunk.Space()\n",
        "        self.space.gravity = Vec2d(0, -1000)\n",
        "\n",
        "        if self.render_sim is True:\n",
        "            self.draw_options = pymunk.pygame_util.DrawOptions(self.screen)\n",
        "            self.draw_options.flags = pymunk.SpaceDebugDrawOptions.DRAW_SHAPES\n",
        "            pymunk.pygame_util.positive_y_is_up = True\n",
        "\n",
        "        #Generating drone's starting position\n",
        "        random_x = random.uniform(200, 600)\n",
        "        random_y = random.uniform(200, 600)\n",
        "        angle_rand = random.uniform(-np.pi/4, np.pi/4)\n",
        "        self.drone = Drone(random_x, random_y, angle_rand, 20, 100, 0.2, 0.4, 0.4, self.space)\n",
        "\n",
        "        self.drone_radius = self.drone.drone_radius\n",
        "\n",
        "    def reward_function(self,obs):\n",
        "        velocity_x = obs[0]\n",
        "        velocity_y =obs[1]\n",
        "        omega = obs[2]\n",
        "        alpha =obs[3]\n",
        "        distance_x = obs[4]\n",
        "        distance_y = obs[5]\n",
        "        pos_x =obs[6]\n",
        "        pos_y = obs[7]\n",
        "\n",
        "        target_pos_x = 0.0\n",
        "        target_pos_y = 0.0\n",
        "\n",
        "        angle_weight = 0.2\n",
        "        distance_weight = 0.5\n",
        "        rotation_weight = 0.1\n",
        "\n",
        "        #velocity_reward = velocity_weight * (velocity_x ** 2 + velocity_y ** 2)\n",
        "        angle_reward = angle_weight*abs(alpha)\n",
        "        distance_reward = distance_weight *np.sqrt( (1.0/(np.abs(obs[4])+0.1)) + (1.0/(np.abs(obs[5])+0.1)))\n",
        "        rotation_reward = rotation_weight * abs(omega)\n",
        "\n",
        "        reward = distance_reward - rotation_reward - angle_reward\n",
        "\n",
        "        return float(reward)\n",
        "\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.first_step is True:\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_drop_path()\n",
        "            if self.render_sim is True and self.render_shade is True: self.add_drone_shade()\n",
        "            self.info = self.initial_movement()\n",
        "\n",
        "        self.left_force = (action[0]/2 + 0.5) * self.froce_scale\n",
        "        self.right_force = (action[1]/2 + 0.5) * self.froce_scale\n",
        "\n",
        "        self.drone.frame_shape.body.apply_force_at_local_point(Vec2d(0, self.left_force), (-self.drone_radius, 0))\n",
        "        self.drone.frame_shape.body.apply_force_at_local_point(Vec2d(0, self.right_force), (self.drone_radius, 0))\n",
        "\n",
        "        self.space.step(1.0/60)\n",
        "        self.current_time_step += 1\n",
        "\n",
        "        #Saving drone's position for drawing\n",
        "        if self.first_step is True:\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_drop_path()\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_flight_path()\n",
        "            self.first_step = False\n",
        "\n",
        "        else:\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_flight_path()\n",
        "\n",
        "        if self.render_sim is True and self.render_shade is True:\n",
        "            x, y = self.drone.frame_shape.body.position\n",
        "            if np.abs(self.shade_x-x) > self.drone_shade_distance or np.abs(self.shade_y-y) > self.drone_shade_distance:\n",
        "                self.add_drone_shade()\n",
        "\n",
        "        #Calulating reward function\n",
        "        obs = self.get_observation()\n",
        "        reward = self.reward_function(obs)\n",
        "        #reward=(1.0/(np.abs(obs[4])+0.1)) + (1.0/(np.abs(obs[5])+0.1))\n",
        "\n",
        "        #Stops episode, when drone is out of range or overlaps\n",
        "        if np.abs(obs[3])==1 or np.abs(obs[6])==1 or np.abs(obs[7])==1:\n",
        "            self.done = True\n",
        "            reward = -10\n",
        "\n",
        "        #Stops episode, when time is up\n",
        "        if self.current_time_step == self.max_time_steps:\n",
        "            self.done = True\n",
        "\n",
        "        return obs, reward, self.done, self.info\n",
        "\n",
        "    def get_observation(self):\n",
        "        velocity_x, velocity_y = self.drone.frame_shape.body.velocity_at_local_point((0, 0))\n",
        "        velocity_x = np.clip(velocity_x/1330, -1, 1)\n",
        "        velocity_y = np.clip(velocity_y/1330, -1, 1)\n",
        "\n",
        "        omega = self.drone.frame_shape.body.angular_velocity\n",
        "        omega = np.clip(omega/11.7, -1, 1)\n",
        "\n",
        "        alpha = self.drone.frame_shape.body.angle\n",
        "        alpha = np.clip(alpha/(np.pi/2), -1, 1)\n",
        "\n",
        "        x, y = self.drone.frame_shape.body.position\n",
        "\n",
        "        if x < self.x_target:\n",
        "            distance_x = np.clip((x/self.x_target) - 1, -1, 0)\n",
        "\n",
        "        else:\n",
        "            distance_x = np.clip((-x/(self.x_target-800) + self.x_target/(self.x_target-800)) , 0, 1)\n",
        "\n",
        "        if y < self.y_target:\n",
        "            distance_y = np.clip((y/self.y_target) - 1, -1, 0)\n",
        "\n",
        "        else:\n",
        "            distance_y = np.clip((-y/(self.y_target-800) + self.y_target/(self.y_target-800)) , 0, 1)\n",
        "\n",
        "        pos_x = np.clip(x/400.0 - 1, -1, 1)\n",
        "        pos_y = np.clip(y/400.0 - 1, -1, 1)\n",
        "\n",
        "        return np.array([velocity_x, velocity_y, omega, alpha, distance_x, distance_y, pos_x, pos_y])\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self.__init__(self.render_sim, self.render_path, self.render_shade, self.drone_shade_distance,\n",
        "                      self.max_time_steps, self.stabilisation_delay, self.change_target, self.initial_throw)\n",
        "        return self.get_observation()\n",
        "\n",
        "    def close(self):\n",
        "        pygame.quit()\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        if self.render_sim is False: return\n",
        "\n",
        "        pygame_events(self.space, self, self.change_target)\n",
        "        self.screen.fill((243, 243, 243))\n",
        "        pygame.draw.rect(self.screen, (24, 114, 139), pygame.Rect(0, 0, 800, 800), 8)\n",
        "        pygame.draw.rect(self.screen, (33, 158, 188), pygame.Rect(50, 50, 700, 700), 4)\n",
        "        pygame.draw.rect(self.screen, (142, 202, 230), pygame.Rect(200, 200, 400, 400), 4)\n",
        "\n",
        "        #Drawing done's shade\n",
        "        if len(self.path_drone_shade):\n",
        "            for shade in self.path_drone_shade:\n",
        "                image_rect_rotated = pygame.transform.rotate(self.shade_image, shade[2]*180.0/np.pi)\n",
        "                shade_image_rect = image_rect_rotated.get_rect(center=(shade[0], 800-shade[1]))\n",
        "                self.screen.blit(image_rect_rotated, shade_image_rect)\n",
        "\n",
        "        self.space.debug_draw(self.draw_options)\n",
        "\n",
        "        #Drawing vectors of motor forces\n",
        "        vector_scale = 0.05\n",
        "        l_x_1, l_y_1 = self.drone.frame_shape.body.local_to_world((-self.drone_radius, 0))\n",
        "        l_x_2, l_y_2 = self.drone.frame_shape.body.local_to_world((-self.drone_radius, self.froce_scale*vector_scale))\n",
        "        pygame.draw.line(self.screen, (179,179,179), (l_x_1, 800-l_y_1), (l_x_2, 800-l_y_2), 4)\n",
        "\n",
        "        l_x_2, l_y_2 = self.drone.frame_shape.body.local_to_world((-self.drone_radius, self.left_force*vector_scale))\n",
        "        pygame.draw.line(self.screen, (255,0,0), (l_x_1, 800-l_y_1), (l_x_2, 800-l_y_2), 4)\n",
        "\n",
        "        r_x_1, r_y_1 = self.drone.frame_shape.body.local_to_world((self.drone_radius, 0))\n",
        "        r_x_2, r_y_2 = self.drone.frame_shape.body.local_to_world((self.drone_radius, self.froce_scale*vector_scale))\n",
        "        pygame.draw.line(self.screen, (179,179,179), (r_x_1, 800-r_y_1), (r_x_2, 800-r_y_2), 4)\n",
        "\n",
        "        r_x_2, r_y_2 = self.drone.frame_shape.body.local_to_world((self.drone_radius, self.right_force*vector_scale))\n",
        "        pygame.draw.line(self.screen, (255,0,0), (r_x_1, 800-r_y_1), (r_x_2, 800-r_y_2), 4)\n",
        "\n",
        "        pygame.draw.circle(self.screen, (255, 0, 0), (self.x_target, 800-self.y_target), 5)\n",
        "\n",
        "        #Drawing drone's path\n",
        "        if len(self.flight_path) > 2:\n",
        "            pygame.draw.aalines(self.screen, (16, 19, 97), False, self.flight_path)\n",
        "\n",
        "        if len(self.drop_path) > 2:\n",
        "            pygame.draw.aalines(self.screen, (255, 0, 0), False, self.drop_path)\n",
        "\n",
        "        pygame.display.flip()\n",
        "        self.clock.tick(60)\n",
        "\n",
        "    def initial_movement(self):\n",
        "        if self.initial_throw is True:\n",
        "            throw_angle = random.random() * 2*np.pi\n",
        "            throw_force = random.uniform(0, 25000)\n",
        "            throw = Vec2d(np.cos(throw_angle)*throw_force, np.sin(throw_angle)*throw_force)\n",
        "\n",
        "            self.drone.frame_shape.body.apply_force_at_world_point(throw, self.drone.frame_shape.body.position)\n",
        "\n",
        "            throw_rotation = random.uniform(-3000, 3000)\n",
        "            self.drone.frame_shape.body.apply_force_at_local_point(Vec2d(0, throw_rotation), (-self.drone_radius, 0))\n",
        "            self.drone.frame_shape.body.apply_force_at_local_point(Vec2d(0, -throw_rotation), (self.drone_radius, 0))\n",
        "\n",
        "            self.space.step(1.0/60)\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_drop_path()\n",
        "\n",
        "        else:\n",
        "            throw_angle = None\n",
        "            throw_force = None\n",
        "            throw_rotation = None\n",
        "\n",
        "        initial_stabilisation_delay = self.stabilisation_delay\n",
        "        while self.stabilisation_delay != 0:\n",
        "            self.space.step(1.0/60)\n",
        "            if self.render_sim is True and self.render_path is True: self.add_postion_to_drop_path()\n",
        "            if self.render_sim is True: self.render()\n",
        "            self.stabilisation_delay -= 1\n",
        "\n",
        "        self.stabilisation_delay = initial_stabilisation_delay\n",
        "\n",
        "        return {'throw_angle': throw_angle, 'throw_force': throw_force, 'throw_rotation': throw_rotation}\n",
        "\n",
        "    def add_postion_to_drop_path(self):\n",
        "        x, y = self.drone.frame_shape.body.position\n",
        "        self.drop_path.append((x, 800-y))\n",
        "\n",
        "    def add_postion_to_flight_path(self):\n",
        "        x, y = self.drone.frame_shape.body.position\n",
        "        self.flight_path.append((x, 800-y))\n",
        "\n",
        "    def add_drone_shade(self):\n",
        "        x, y = self.drone.frame_shape.body.position\n",
        "        self.path_drone_shade.append([x, y, self.drone.frame_shape.body.angle])\n",
        "        self.shade_x = x\n",
        "        self.shade_y = y\n",
        "\n",
        "    def change_target_point(self, x, y):\n",
        "        self.x_target = x\n",
        "        self.y_target = y"
      ],
      "metadata": {
        "id": "q22KDkf-iNA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing Proximal Policy Optimization A2C with pytorch\n"
      ],
      "metadata": {
        "id": "8MaPP92zj84l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjjs8VrcilSV"
      },
      "outputs": [],
      "source": [
        "#Importing Neccessary Libraries for the model\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import gym\n",
        "#-----------pytorch libraries----------------#\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions import MultivariateNormal\n",
        "from scipy.stats import multivariate_normal\n",
        "from torch.optim import Adam"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing the Proximal Policy Optimization\n",
        "#The below class will store the local variables for clean policy\n",
        "\n",
        "class store:\n",
        "  def __init__(self):\n",
        "    self.actions=[]\n",
        "    self.states=[]\n",
        "    self.log_probs=[]\n",
        "    self.rewards=[]\n",
        "    self.state_values=[]\n",
        "    self.terminal=[]\n",
        "\n",
        "  def clear(self):\n",
        "    del self.actions[:]\n",
        "    del self.states[:]\n",
        "    del self.log_probs[:]\n",
        "    del self.rewards[:]\n",
        "    del self.state_values[:]\n",
        "    del self.terminal[:]\n",
        "\n",
        "#Implementing A2C\n",
        "#since we are dealing with Drone environment with continuous space of action\n",
        "#i.e [0,1]^4\n",
        "\n",
        "class Actor_Critic(nn.Module):\n",
        "  def __init__(self,action_dim,state_dim,action_dist):\n",
        "    super(Actor_Critic,self).__init__()\n",
        "\n",
        "    self.action_var=torch.full((action_dim,),action_dist*action_dist).to(device)\n",
        "    self.action_dim = action_dim\n",
        "    #Actor Network\n",
        "    self.Actor=nn.Sequential(\n",
        "        nn.Linear(state_dim,84),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(84,84),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(84,action_dim),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "    #Critic Network\n",
        "    self.Critic=nn.Sequential(\n",
        "        nn.Linear(state_dim,84),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(84,84),\n",
        "        nn.Tanh(),\n",
        "        nn.Linear(84,1)\n",
        "    )\n",
        "\n",
        "  def setup_action_dist(self,new_action_dist):\n",
        "    #since we are dealing continuous space action we need to keep on changing the\n",
        "    #action dist to make our model learn new things\n",
        "    self.action_var=torch.full((self.action_dim,),new_action_dist*new_action_dist).to(device)\n",
        "\n",
        "  def action_pred(self,obs):\n",
        "    action_mean=self.Actor(obs)\n",
        "    cov_mat=torch.diag(self.action_var).unsqueeze(dim=0)\n",
        "    prob_distribution=MultivariateNormal(action_mean,cov_mat)\n",
        "    action=prob_distribution.sample()\n",
        "    log_probs=prob_distribution.log_prob(action)\n",
        "    state_vals=self.Critic(obs)\n",
        "\n",
        "    return action.detach(),log_probs.detach(),state_vals.detach()\n",
        "\n",
        "  def evaluate(self,obs,action):\n",
        "    values=self.Critic(obs)\n",
        "    action_mean=self.Actor(obs)\n",
        "    cov_mat=torch.diag_embed(self.action_var.expand_as(action_mean))\n",
        "    prob_distribution=MultivariateNormal(action_mean,cov_mat)\n",
        "    log_probs=prob_distribution.log_prob(action)\n",
        "\n",
        "    return log_probs,values\n",
        "\n",
        "\n",
        "#From here we will implement the clipped policy\n",
        "class PPO:\n",
        "  def __init__(self,action_dim,state_dim,epoch,lr_actor,lr_critic,clip,gamma,action_dist):\n",
        "    self.states_dim=state_dim\n",
        "    self.action_dim=action_dim\n",
        "    self.action_dist=action_dist\n",
        "    self.epoch=epoch\n",
        "    self.clip=clip\n",
        "    self.gamma=gamma\n",
        "    self.lr_actor=lr_actor\n",
        "    self.lr_critic=lr_critic\n",
        "    self.local=store()\n",
        "    self.policy=Actor_Critic(self.action_dim,self.states_dim,action_dist).to(device)\n",
        "    self.optimizer=torch.optim.Adam([\n",
        "        {'params':self.policy.Actor.parameters(),'lr':self.lr_actor},\n",
        "        {'params':self.policy.Critic.parameters(),'lr':self.lr_critic}\n",
        "    ])\n",
        "    self.policy_old=Actor_Critic(self.action_dim,self.states_dim,action_dist).to(device)\n",
        "    self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "\n",
        "  def set_action_dist_policy(self,new_action_dist):\n",
        "    self.action_dist=new_action_dist\n",
        "    self.policy.setup_action_dist(new_action_dist)\n",
        "    self.policy_old.setup_action_dist(new_action_dist)\n",
        "\n",
        "  def decay_action_dist(self,action_dist_decay_rate,min_action_dist):\n",
        "    print(\"-----------setting-up-action-decay-rate-------------\")\n",
        "    self.action_dist=self.action_dist-action_dist_decay_rate\n",
        "    if(self.action_dist<=min_action_dist):\n",
        "      self.action_dist=min_action_dist\n",
        "      print(\"setting the action dist to min_action_dist:\",min_action_dist)\n",
        "\n",
        "    else:\n",
        "      print(\"setting the action dist to: \",self.action_dist)\n",
        "\n",
        "    self.set_action_dist_policy(self.action_dist)\n",
        "\n",
        "  def pick_action(self,obs):\n",
        "    with torch.no_grad():\n",
        "      obs=torch.FloatTensor(obs).to(device)\n",
        "      action,logprob,state_val=self.policy_old.action_pred(obs)\n",
        "\n",
        "    self.local.actions.append(action)\n",
        "    self.local.log_probs.append(logprob)\n",
        "    self.local.states.append(obs)\n",
        "    self.local.state_values.append(state_val)\n",
        "\n",
        "    return action.detach().cpu().numpy().flatten()\n",
        "\n",
        "  def update(self):\n",
        "    rewards=[]\n",
        "    discounted_reward=0\n",
        "    for reward, terminal in zip(reversed(self.local.rewards),reversed(self.local.terminal)):\n",
        "        if terminal:\n",
        "            discounted_reward=0\n",
        "        discounted_reward=reward+(self.gamma*discounted_reward)\n",
        "        rewards.insert(0,discounted_reward)\n",
        "\n",
        "    rewards=torch.tensor(rewards,dtype=torch.float32).to(device)\n",
        "    rewards=(rewards-rewards.mean())/(rewards.std()+1e-7)\n",
        "\n",
        "    stack_states=torch.squeeze(torch.stack(self.local.states,dim=0)).detach().to(device)\n",
        "    stack_actions=torch.squeeze(torch.stack(self.local.actions,dim=0)).detach().to(device)\n",
        "    stack_log_probs=torch.squeeze(torch.stack(self.local.log_probs,dim=0)).detach().to(device)\n",
        "    stack_state_values=torch.squeeze(torch.stack(self.local.state_values,dim=0)).detach().to(device)\n",
        "\n",
        "    advantages=rewards.detach()-stack_state_values.detach()\n",
        "\n",
        "    for i in range(self.epoch):\n",
        "      logprobs,state_val=self.policy.evaluate(stack_states,stack_actions)\n",
        "      state_val=torch.squeeze(state_val)\n",
        "      ratio=torch.exp(logprobs-stack_log_probs)\n",
        "      s1=ratio*advantages\n",
        "      s2=torch.clamp(ratio,1-self.clip,1+self.clip)*advantages\n",
        "\n",
        "      loss=-torch.min(s1,s2) +0.5*nn.MSELoss()(state_val,rewards)\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()\n",
        "\n",
        "    self.policy_old.load_state_dict(self.policy.state_dict())\n",
        "    self.local.clear()\n",
        "\n",
        "  def save(self,path):\n",
        "    torch.save(self.policy_old.state_dict(),path)\n",
        "\n",
        "  def load(self,path):\n",
        "    self.policy_old.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage))\n",
        "    self.policy.load_state_dict(torch.load(path, map_location=lambda storage, loc: storage))\n"
      ],
      "metadata": {
        "id": "3d0Zcb88VQWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    from gym.envs.registration import register\n",
        "\n",
        "    register(\n",
        "        id='drone-2d-custom-v0',\n",
        "        entry_point='__main__:Drone2dEnv',\n",
        "        kwargs={'render_sim': False, 'render_path': True, 'render_shade': True,\n",
        "                'shade_distance': 75, 'n_steps': 500, 'n_fall_steps': 10, 'change_target': False,\n",
        "                'initial_throw': True}\n",
        "    )\n",
        "\n",
        "    # Create an instance of the Drone2dEnv class\n",
        "    env = Drone2dEnv()\n",
        "\n",
        "    # Perform any necessary initialization or setup here\n",
        "\n",
        "    # Run the environment loop\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        action = env.action_space.sample()  # Replace with your desired action selection logic\n",
        "        observation, reward, done, info = env.step(action)\n",
        "          # Optional: Render the environment"
      ],
      "metadata": {
        "id": "YrRANSbqueau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting up the process to cpu to cuda, please select GPU in runtime,\n",
        "#if you are doing it in colab!\n",
        "print(\"-----------setting-up-device-------------\")\n",
        "if(torch.cuda.is_available()):\n",
        "  device=torch.device('cuda:0')\n",
        "  torch.cuda.empty_cache() #clearing out the cache in the begining\n",
        "  print(\"The process will be fast as Device Is Set to: \"+ str(torch.cuda.get_device_name(device)))\n",
        "\n",
        "else:\n",
        "  device=torch.device('cpu')\n",
        "  print(\"The process will be slow as Device is set to: CPU\")\n",
        "print(\"-----------------------------------------\")\n",
        "def train():\n",
        "  print(\"-------initiating training of policy-----\")\n",
        "  env_name=\"Drone_RL_PPO_V1.2\"\n",
        "  print(\"-------Env has continuos action space----\")\n",
        "  max_epi_len=1500\n",
        "  max_training_timestep=int(2e6)\n",
        "\n",
        "  print_freq=max_epi_len*10\n",
        "  log_freq=max_epi_len*2\n",
        "  save_model_freq=int(5e5)\n",
        "\n",
        "  action_dist=0.6\n",
        "  action_dist_decay_rate=0.06\n",
        "  min_action_dist=0.1\n",
        "  action_std_decay_freq=int(2e5)\n",
        "\n",
        "  update_timestep=max_epi_len*4\n",
        "  epoch=60\n",
        "  clip=0.2\n",
        "  gamma=0.98\n",
        "  lr_actor=0.0003\n",
        "  lr_critic=0.001\n",
        "  seed=0\n",
        "  print(\"-----------------------------------------\")\n",
        "  print(\"ENV name: \"+env_name)\n",
        "  state_dim=env.observation_space.shape[0]\n",
        "  action_dim=env.action_space.shape[0]\n",
        "  log_dir=\"PPO_LOGS\"\n",
        "  if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "  log_dir = log_dir + '/' + env_name + '/'\n",
        "  if not os.path.exists(log_dir):\n",
        "        os.makedirs(log_dir)\n",
        "\n",
        "  log_f_name=log_dir+env_name+\"_log_01\"+\".csv\"\n",
        "  print(\"logging at: \"+log_f_name)\n",
        "\n",
        "  num_pre=0\n",
        "  directory = \"PPO_preTrained\"\n",
        "  if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "  directory = directory + '/' + env_name + '/'\n",
        "  if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "  path=directory+\"PPO_{}_{}.pth\".format(env_name,num_pre)\n",
        "  print(\"model saving at: \" +path)\n",
        "\n",
        "  torch.manual_seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  env.seed(seed)\n",
        "  print(\"-----------------------------------------\")\n",
        "\n",
        "  agent=PPO(action_dim,state_dim,epoch,lr_actor,lr_critic,clip,gamma,action_dist)\n",
        "  start=datetime.now()\n",
        "  print(\"started training time at: \",start)\n",
        "  logger=open(log_f_name,\"w+\")\n",
        "  logger.write('Episode,Timestep,Reward\\n')\n",
        "\n",
        "  log_running_reward=0\n",
        "  log_running_episodes=0\n",
        "  print_running_reward=0\n",
        "  print_running_episodes=0\n",
        "  time_step=0\n",
        "  i_episode=0\n",
        "\n",
        "  while time_step<=max_training_timestep:\n",
        "    state=env.reset()\n",
        "    current_episode_reward=0\n",
        "    for i in range(1,max_epi_len+1):\n",
        "      action=agent.pick_action(state)\n",
        "      state,reward,done,_=env.step(action)\n",
        "      agent.local.rewards.append(reward)\n",
        "      agent.local.terminal.append(done)\n",
        "      time_step+=1\n",
        "      current_episode_reward+=reward\n",
        "\n",
        "      if time_step % update_timestep==0:\n",
        "        agent.update()\n",
        "\n",
        "      if time_step % action_std_decay_freq==0:\n",
        "        agent.decay_action_dist(action_dist_decay_rate,min_action_dist)\n",
        "\n",
        "      if time_step % log_freq==0:\n",
        "        log_avg_reward=log_running_reward / log_running_episodes\n",
        "        log_avg_reward=round(log_avg_reward,4)\n",
        "\n",
        "        logger.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n",
        "        logger.flush()\n",
        "\n",
        "        log_running_reward=0\n",
        "        log_running_episodes=0\n",
        "\n",
        "      if time_step % print_freq==0:\n",
        "        print_avg_reward=print_running_reward / print_running_episodes\n",
        "        print_avg_reward=round(print_avg_reward, 2)\n",
        "\n",
        "        print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
        "\n",
        "        print_running_reward=0\n",
        "        print_running_episodes=0\n",
        "\n",
        "      if time_step % save_model_freq==0:\n",
        "        print(\"-----------------------------------------\")\n",
        "        print(\"saving model at: \"+path)\n",
        "        agent.save(path)\n",
        "        print(\"model saved\")\n",
        "        print(\"Time taken: \",datetime.now().replace(microsecond=0) - start)\n",
        "        print(\"-----------------------------------------\")\n",
        "\n",
        "      if done:\n",
        "        break\n",
        "    print_running_reward+=current_episode_reward\n",
        "    print_running_episodes+=1\n",
        "    log_running_reward += current_episode_reward\n",
        "    log_running_episodes += 1\n",
        "    i_episode += 1\n",
        "\n",
        "  logger.close()\n",
        "  env.close()\n",
        "  print(\"-----------------------------------------\")\n",
        "  end_time = datetime.now().replace(microsecond=0)\n",
        "  print(\"Started training at (GMT) : \", start)\n",
        "  print(\"Finished training at (GMT) : \", end_time)\n",
        "  print(\"Total training time  : \", end_time - start)\n",
        "  print(\"-----------------------------------------\")\n",
        "\n",
        "if __name__ =='__main__':\n",
        "  train()"
      ],
      "metadata": {
        "id": "1bXfR5s_aBhD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}